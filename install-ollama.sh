#!/bin/bash

# Comprehensive Ollama Auto-Installation Script
# This script ensures Ollama is installed and configured for the Medical AI Assistant

set -e

# Parse command line arguments
AUTO_MODE=false
FULL_SETUP=false
QUIET_MODE=false

for arg in "$@"; do
  case $arg in
    --auto)
      AUTO_MODE=true
      shift
      ;;
    --full)
      FULL_SETUP=true
      shift
      ;;
    --quiet)
      QUIET_MODE=true
      shift
      ;;
    *)
      ;;
  esac
done

# Logging functions
log() {
  if [ "$QUIET_MODE" = false ]; then
    echo "$1"
  fi
}

log "üè• Medical AI Assistant - Ollama Auto-Setup"
log "========================================="

# Detect OS
OS=""
ARCH=""
case "$OSTYPE" in
  linux*)   OS="linux" ;;
  darwin*)  OS="darwin" ;;
  msys*|win32*) OS="windows" ;;
  *)        echo "‚ùå Unsupported OS: $OSTYPE" && exit 1 ;;
esac

case "$(uname -m)" in
  x86_64)   ARCH="amd64" ;;
  arm64|aarch64) ARCH="arm64" ;;
  *)        log "‚ùå Unsupported architecture: $(uname -m)" && exit 1 ;;
esac

log "üîç Detected: $OS/$ARCH"

# Function to check if Ollama is installed
check_ollama_installed() {
  if command -v ollama &> /dev/null; then
    log "‚úÖ Ollama is already installed"
    if [ "$QUIET_MODE" = false ]; then
      ollama --version
    fi
    return 0
  else
    log "‚ùå Ollama not found"
    return 1
  fi
}

# Function to install Ollama
install_ollama() {
  log "üì• Installing Ollama..."
  
  case "$OS" in
    "linux"|"darwin")
      # Use official installer
      if [ "$QUIET_MODE" = true ]; then
        curl -fsSL https://ollama.ai/install.sh | sh > /dev/null 2>&1
      else
        curl -fsSL https://ollama.ai/install.sh | sh
      fi
      ;;
    "windows")
      log "ü™ü For Windows, please download Ollama manually from https://ollama.ai/download"
      log "   Then run this script again"
      if [ "$AUTO_MODE" = true ]; then
        log "‚ö†Ô∏è  Auto-installation not supported on Windows"
        return 0
      fi
      exit 1
      ;;
  esac
  
  # Verify installation
  if command -v ollama &> /dev/null; then
    log "‚úÖ Ollama installed successfully"
    if [ "$QUIET_MODE" = false ]; then
      ollama --version
    fi
  else
    log "‚ùå Failed to install Ollama"
    exit 1
  fi
}

# Function to start Ollama service
start_ollama_service() {
  echo "üöÄ Starting Ollama service..."
  
  # Check if already running
  if curl -s http://localhost:11434/api/tags &> /dev/null; then
    echo "‚úÖ Ollama service is already running"
    return 0
  fi
  
  # Start Ollama in background
  if command -v systemctl &> /dev/null && systemctl is-enabled ollama &> /dev/null; then
    # Use systemd if available
    sudo systemctl start ollama
  else
    # Start manually
    echo "Starting Ollama manually..."
    nohup ollama serve > /tmp/ollama.log 2>&1 &
    
    # Wait for service to start
    echo "‚è≥ Waiting for Ollama to start..."
    for i in {1..30}; do
      if curl -s http://localhost:11434/api/tags &> /dev/null; then
        echo "‚úÖ Ollama service started successfully"
        return 0
      fi
      sleep 1
    done
    
    echo "‚ùå Failed to start Ollama service"
    echo "Check logs: tail /tmp/ollama.log"
    exit 1
  fi
}

# Function to download required models
download_models() {
  log "ü§ñ Setting up AI models..."
  
  # Check if tinyllama is available
  if ollama list | grep -q "tinyllama:latest"; then
    log "‚úÖ TinyLlama model is already available"
  else
    log "üì• Downloading TinyLlama model (this may take a few minutes)..."
    if [ "$QUIET_MODE" = true ]; then
      ollama pull tinyllama:latest > /dev/null 2>&1
    else
      ollama pull tinyllama:latest
    fi
    log "‚úÖ TinyLlama model downloaded"
  fi
  
  # Optional: Download a better model if user wants or if full setup requested
  if [ "$FULL_SETUP" = true ]; then
    log "üì• Downloading Llama 3.1 8B model (full setup mode)..."
    if [ "$QUIET_MODE" = true ]; then
      ollama pull llama3.1:8b > /dev/null 2>&1
    else
      ollama pull llama3.1:8b
    fi
    log "‚úÖ Llama 3.1 8B model downloaded"
  elif [ "$AUTO_MODE" = false ]; then
    read -p "üöÄ Would you like to download a larger, more capable model (llama3.1:8b - 4.9GB)? [y/N]: " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
      log "üì• Downloading Llama 3.1 8B model (this will take longer)..."
      ollama pull llama3.1:8b
      log "‚úÖ Llama 3.1 8B model downloaded"
    fi
  fi
}

# Function to test the setup
test_setup() {
  echo "üß™ Testing Ollama setup..."
  
  # Test API connection
  if ! curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "‚ùå Cannot connect to Ollama API"
    return 1
  fi
  
  # Test model generation
  echo "Testing AI response..."
  RESPONSE=$(curl -s http://localhost:11434/api/generate -d '{
    "model": "tinyllama:latest",
    "prompt": "What is a headache?",
    "stream": false
  }' | grep -o '"response":"[^"]*"' | sed 's/"response":"\(.*\)"/\1/' || echo "")
  
  if [ -n "$RESPONSE" ]; then
    echo "‚úÖ AI is responding correctly"
    echo "Sample response: ${RESPONSE:0:100}..."
  else
    echo "‚ùå AI is not responding"
    return 1
  fi
}

# Function to create auto-start configuration
setup_autostart() {
  log "‚öôÔ∏è  Setting up auto-start configuration..."
  
  if command -v systemctl &> /dev/null; then
    # Create systemd service for Linux
    cat > /tmp/ollama.service << EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=$USER
Group=$USER
Restart=always
RestartSec=3
Environment="HOME=$HOME"
Environment="PATH=$PATH"

[Install]
WantedBy=default.target
EOF
    
    log "Creating systemd service..."
    if sudo cp /tmp/ollama.service /etc/systemd/system/ 2>/dev/null; then
      sudo systemctl daemon-reload
      sudo systemctl enable ollama
      log "‚úÖ Ollama will start automatically on boot"
    else
      log "‚ö†Ô∏è  Could not set up autostart (requires sudo)"
    fi
  else
    log "‚ÑπÔ∏è  Manual start required: run 'ollama serve' before using the Medical AI Assistant"
  fi
}

# Main installation flow
main() {
  if [ "$AUTO_MODE" = true ]; then
    log "ü§ñ Running in automatic mode..."
  fi
  
  log ""
  
  # Step 1: Check/Install Ollama
  if ! check_ollama_installed; then
    if [ "$AUTO_MODE" = true ] || [ "$FULL_SETUP" = true ]; then
      install_ollama
    else
      read -p "Would you like to install Ollama now? [Y/n]: " -n 1 -r
      echo
      if [[ $REPLY =~ ^[Nn]$ ]]; then
        log "‚ùå Ollama installation skipped. The Medical AI Assistant requires Ollama to function."
        exit 1
      fi
      install_ollama
    fi
  fi
  
  # Step 2: Start Ollama service
  start_ollama_service
  
  # Step 3: Download models
  download_models
  
  # Step 4: Test setup
  if test_setup; then
    log "‚úÖ All tests passed!"
  else
    log "‚ö†Ô∏è  Setup completed but tests failed. Please check manually."
  fi
  
  # Step 5: Setup auto-start (optional, automatic in auto mode)
  if [ "$AUTO_MODE" = true ] || [ "$FULL_SETUP" = true ]; then
    setup_autostart
  else
    read -p "üîÑ Would you like Ollama to start automatically on boot? [y/N]: " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
      setup_autostart
    fi
  fi
  
  log ""
  log "üéâ Medical AI Assistant setup complete!"
  log ""
  log "üìã Summary:"
  log "   ‚Ä¢ Ollama installed: ‚úÖ"
  log "   ‚Ä¢ Service running: ‚úÖ"
  log "   ‚Ä¢ Models downloaded: ‚úÖ"
  log "   ‚Ä¢ API tested: ‚úÖ"
  log ""
  if [ "$AUTO_MODE" = false ]; then
    log "üöÄ You can now run the Medical AI Assistant!"
    log "   npm run dev"
  fi
  log ""
}

# Run main function
main "$@"
